from datetime import datetime
import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
import pickle
import time
import json
import sys
import os
import platform
import subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'catboost==1.2.8'])
import catboost as catb
from prefect import flow
import argparse

@flow
def preprocess_dataframe(df):
    for col in df.columns:
        if df[col].dtype == 'bool':
            df[col] = df[col].astype(int)
        elif df[col].dtype in ['int16', 'int32', 'int64']:
            df[col] = df[col].astype(int)
        elif df[col].dtype in ['float16', 'float32', 'float64']:
            # floats con solo 4 decimales
            df[col] = df[col].astype(float).round(4)
    return df

@flow
def save_model(model, ml_name, performance, params, save_dir):
    # Save the model
    model_filename = f'{ml_name}_model.pkl'
    model_path = f'{save_dir}/{model_filename}'
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    print(f"Modelo guardado en: {model_path}")

    # Save metadata
    metadata_filename = f'{ml_name}_metadata.json'
    metadata_path = f'{save_dir}/{metadata_filename}'

    # Get library versions
    library_versions = {
        'xgboost': xgb.__version__ if ml_name == 'xgb' else None,
        'lightgbm': lgb.__version__ if ml_name == 'lgbm' else None,
        'catboost': catb.__version__ if ml_name == 'catb' else None,
        'pandas': pd.__version__,
        'numpy': np.__version__,
        'scikit-learn': platform.version()}

    metadata = {
        'ml_name': ml_name,
        'performance': performance,
        'hyperparameters': params,
        'library_versions': library_versions,
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}

    # Limpiar parámetros para la serialización JSON (por ejemplo, manejar tipos no serializables)
    def clean_params(p):
        cleaned = {}
        for k, v in p.items():
            try:
                json.dumps(v) # Check if serializable
                cleaned[k] = v
            except (TypeError, OverflowError):
                cleaned[k] = str(v)
        return cleaned

    metadata['hyperparameters'] = clean_params(metadata['hyperparameters'])
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=4)
    print(f"Metadata guardada en: {metadata_path}")

def train(train_path, test_path, model_save_dir):
    now = datetime.now()
    folder_name = now.strftime("%Y-%m-%d_%H-%M-%S")
    model_save_dir = f'{model_save_dir}/{folder_name}'
    os.makedirs(model_save_dir, exist_ok=True)
    print(f"Directorio de modelos guardado en: {model_save_dir}")
    # cargar data
    try:
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
    except FileNotFoundError:
        print(f"Error: No se encontró el archivo del conjunto de datos. Revisar las rutas: {train_path}, {test_path}")
        return

    # Preprocess data
    train_df = preprocess_dataframe(train_df)
    test_df = preprocess_dataframe(test_df)

    # Separar features y target
    X_train = train_df.drop(columns=[train_df.columns[0]]) # type: ignore
    y_train = train_df[train_df.columns[0]] # type: ignore
    X_test = test_df.drop(columns=[test_df.columns[0]]) # type: ignore
    y_test = test_df[test_df.columns[0]] # type: ignore

    # Asegurarse que las columnas de train y test coincidan
    train_cols = X_train.columns
    test_cols = X_test.columns

    missing_in_test = set(train_cols) - set(test_cols)
    for c in missing_in_test:
        X_test[c] = 0

    missing_in_train = set(test_cols) - set(train_cols)
    for c in missing_in_train:
        X_train[c] = 0

    X_test = X_test[train_cols]

    # Model training and evaluation
    models = {
        'xgb': {
            'model': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
            'params': {}, # parametros por default
            'performance': {}},
        'lgbm': {
            'model': lgb.LGBMClassifier(random_state=42),
            'params': {}, # parametros por default
            'performance': {}},
        'catb': {
            'model': catb.CatBoostClassifier(verbose=0, random_state=42),
            'params': {}, # parametros por default
            'performance': {}}
    }

    best_ml_name = None
    best_auc_test = -np.inf
    best_decay = np.inf

    for ml_name, model_info in models.items():
        model = model_info['model']
        start_time = time.time()
        if ml_name == 'catb':
             # CatBoost puede manejar diferentes tipos de datos, pero usemos los datos preprocesados
             model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=10, verbose=0)
        else:
            model.fit(X_train, y_train)
        end_time = time.time()
        training_time = end_time - start_time

        y_train_pred_proba = model.predict_proba(X_train)[:, 1]
        y_test_pred_proba = model.predict_proba(X_test)[:, 1]

        auc_train = roc_auc_score(y_train, y_train_pred_proba)
        auc_test = roc_auc_score(y_test, y_test_pred_proba)
        decay = ((auc_train - auc_test) / auc_train) * 100 if auc_train > 0 else np.inf

        models[ml_name]['performance'] = {
            'auc_train': auc_train,
            'auc_test': auc_test,
            'decay_percent': decay,
            'training_time_segs': training_time}

        models[ml_name]['params'] = model.get_params()

        print(f"Model: {ml_name}")
        print(f"  AUC Train: {auc_train:.4f}")
        print(f"  AUC Test: {auc_test:.4f}")
        print(f"  Decay (%): {decay:.2f}%")
        print("-" * 20)

        # Check if this model is the champion
        if auc_test > best_auc_test and decay < 10:
            best_auc_test = auc_test
            best_decay = decay
            best_ml_name = ml_name

    if best_ml_name:
        feature_names = list(X_train.columns)
        with open(f"{model_save_dir}/feature_names.json", "w") as f:
            json.dump(feature_names, f)

        print(f"\nModelo finalista: {best_ml_name}")
        save_model(models[best_ml_name]['model'], best_ml_name, models[best_ml_name]['performance'], models[best_ml_name]['params'], model_save_dir)
    else:
        print("\nNo se encontró ningún modelo campeón que cumpla con los criterios (Decaimiento < 10%).")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("output", type=str, help="directory saving model")
    parser.add_argument("--train-path", type=str)
    parser.add_argument("--test-path", type=str)
    args = parser.parse_args()

    train(args.train_path, args.test_path, args.output)
